{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      video_id                                           comments  \\\n",
      "0  4sbklcQ0EXc                                     tri clerk free   \n",
      "1  4sbklcQ0EXc  start work mern microservic microfrontend arch...   \n",
      "2  4sbklcQ0EXc  hey add featur join friend listen along know w...   \n",
      "3  4sbklcQ0EXc  hello bro get bug clerkmiddlewar appus clerkmi...   \n",
      "4  4sbklcQ0EXc  next project idea mern notion clone rtk queri ...   \n",
      "\n",
      "                                               title  \n",
      "0  Advanced Spotify Clone: Build & Deploy a MERN ...  \n",
      "1  Advanced Spotify Clone: Build & Deploy a MERN ...  \n",
      "2  Advanced Spotify Clone: Build & Deploy a MERN ...  \n",
      "3  Advanced Spotify Clone: Build & Deploy a MERN ...  \n",
      "4  Advanced Spotify Clone: Build & Deploy a MERN ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"file_with_titles.csv\")  # Replace with the path to your dataset\n",
    "\n",
    "# Verify dataset structure\n",
    "print(df.head())\n",
    "\n",
    "# Ensure columns are properly named\n",
    "assert \"video_id\" in df.columns and \"comments\" in df.columns and \"title\" in df.columns\n",
    "\n",
    "# Combine video titles with comments as pairs\n",
    "df['text_pair'] = df['comments'] + \" [SEP] \" + df['title']\n",
    "\n",
    "# Add dummy labels (you can replace this with actual relevance scores or binary labels if available)\n",
    "df['label'] = 1.0  # Assuming all pairs are relevant for now; modify as per your use case\n",
    "\n",
    "df['comment_length'] = df['comments'].apply(len)\n",
    "plt.hist(df['comment_length'], bins=30, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Distribution of Comment Lengths\")\n",
    "plt.xlabel(\"Comment Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Train-test split\n",
    "train_df, val_df = train_test_split(df[['text_pair', 'label']], test_size=0.1, random_state=42)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (3.1.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from datasets) (3.11.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from datasets) (4.67.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from datasets) (2024.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.17.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (4.46.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: requests in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: xxhash in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from datasets) (3.11.2)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from datasets) (2024.9.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.17.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16c1bd8dae54e79a32635d5a0e623aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23833 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7638b864c994414691dcddf228aa09c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2649 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor(1.), '__index_level_0__': tensor(7653), 'input_ids': tensor([    0, 14406,  1994,  3157,   646,  3388,   510,   742,  1534,   256,\n",
      "        29092, 31197,   208, 38680,    11,   666,   116, 33958, 11649,  1721,\n",
      "         1018,  5245,  8771,    11, 15294,     2,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/paraphrase-distilroberta-base-v1\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the text pairs\n",
    "    return tokenizer(examples['text_pair'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Apply the tokenization function to the training and validation sets\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove the 'text_pair' column as it is no longer needed\n",
    "train_dataset = train_dataset.remove_columns([\"text_pair\"])\n",
    "val_dataset = val_dataset.remove_columns([\"text_pair\"])\n",
    "\n",
    "# Rename 'label' column to match Hugging Face's expectations\n",
    "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
    "val_dataset = val_dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "train_dataset.set_format(\"torch\")\n",
    "val_dataset.set_format(\"torch\")\n",
    "\n",
    "# Visualize token length distribution\n",
    "train_token_lengths = [len(tokenizer(example['text_pair'])['input_ids']) for example in train_df.to_dict(orient='records')]\n",
    "val_token_lengths = [len(tokenizer(example['text_pair'])['input_ids']) for example in val_df.to_dict(orient='records')]\n",
    "\n",
    "plt.hist(train_token_lengths, bins=30, alpha=0.7, label='Train Dataset', color='blue', edgecolor='black')\n",
    "plt.hist(val_token_lengths, bins=30, alpha=0.7, label='Validation Dataset', color='green', edgecolor='black')\n",
    "plt.title(\"Token Length Distribution\")\n",
    "plt.xlabel(\"Number of Tokens\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Verify tokenized dataset\n",
    "print(train_dataset[0])  # Print the first entry of the training dataset to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Aditya Agarwal\\anaconda3\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/paraphrase-distilroberta-base-v1 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Aditya Agarwal\\anaconda3\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a42d8a4c8346df85bcc36a291b6fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4470 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0434, 'grad_norm': 0.698472261428833, 'learning_rate': 1.9776286353467565e-05, 'epoch': 0.03}\n",
      "{'loss': 0.0078, 'grad_norm': 0.7430528402328491, 'learning_rate': 1.9552572706935124e-05, 'epoch': 0.07}\n",
      "{'loss': 0.0066, 'grad_norm': 0.6802736520767212, 'learning_rate': 1.9328859060402687e-05, 'epoch': 0.1}\n",
      "{'loss': 0.0074, 'grad_norm': 0.3465728461742401, 'learning_rate': 1.9105145413870246e-05, 'epoch': 0.13}\n",
      "{'loss': 0.0053, 'grad_norm': 0.8693322539329529, 'learning_rate': 1.888143176733781e-05, 'epoch': 0.17}\n",
      "{'loss': 0.0048, 'grad_norm': 0.20168718695640564, 'learning_rate': 1.865771812080537e-05, 'epoch': 0.2}\n",
      "{'loss': 0.0049, 'grad_norm': 0.6426231861114502, 'learning_rate': 1.8434004474272932e-05, 'epoch': 0.23}\n",
      "{'loss': 0.0041, 'grad_norm': 0.24735701084136963, 'learning_rate': 1.8210290827740495e-05, 'epoch': 0.27}\n",
      "{'loss': 0.0037, 'grad_norm': 0.550957441329956, 'learning_rate': 1.7986577181208054e-05, 'epoch': 0.3}\n",
      "{'loss': 0.0033, 'grad_norm': 0.4024861752986908, 'learning_rate': 1.7762863534675617e-05, 'epoch': 0.34}\n",
      "{'loss': 0.0031, 'grad_norm': 0.4586867690086365, 'learning_rate': 1.753914988814318e-05, 'epoch': 0.37}\n",
      "{'loss': 0.0036, 'grad_norm': 0.5714172124862671, 'learning_rate': 1.731543624161074e-05, 'epoch': 0.4}\n",
      "{'loss': 0.003, 'grad_norm': 0.43261757493019104, 'learning_rate': 1.7091722595078302e-05, 'epoch': 0.44}\n",
      "{'loss': 0.0031, 'grad_norm': 0.3352888226509094, 'learning_rate': 1.6868008948545862e-05, 'epoch': 0.47}\n",
      "{'loss': 0.0028, 'grad_norm': 0.29212331771850586, 'learning_rate': 1.6644295302013425e-05, 'epoch': 0.5}\n",
      "{'loss': 0.0025, 'grad_norm': 0.30218401551246643, 'learning_rate': 1.6420581655480984e-05, 'epoch': 0.54}\n",
      "{'loss': 0.0025, 'grad_norm': 0.22710973024368286, 'learning_rate': 1.6196868008948547e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0028, 'grad_norm': 0.7315330505371094, 'learning_rate': 1.5973154362416107e-05, 'epoch': 0.6}\n",
      "{'loss': 0.0024, 'grad_norm': 0.4134047031402588, 'learning_rate': 1.574944071588367e-05, 'epoch': 0.64}\n",
      "{'loss': 0.0022, 'grad_norm': 0.18681290745735168, 'learning_rate': 1.552572706935123e-05, 'epoch': 0.67}\n",
      "{'loss': 0.0022, 'grad_norm': 0.10777731239795685, 'learning_rate': 1.5302013422818792e-05, 'epoch': 0.7}\n",
      "{'loss': 0.0025, 'grad_norm': 1.2851706743240356, 'learning_rate': 1.5078299776286353e-05, 'epoch': 0.74}\n",
      "{'loss': 0.0024, 'grad_norm': 0.205585315823555, 'learning_rate': 1.4854586129753916e-05, 'epoch': 0.77}\n",
      "{'loss': 0.002, 'grad_norm': 0.5229939818382263, 'learning_rate': 1.4630872483221479e-05, 'epoch': 0.81}\n",
      "{'loss': 0.0021, 'grad_norm': 0.25750046968460083, 'learning_rate': 1.4407158836689039e-05, 'epoch': 0.84}\n",
      "{'loss': 0.0021, 'grad_norm': 0.6401766538619995, 'learning_rate': 1.4183445190156601e-05, 'epoch': 0.87}\n",
      "{'loss': 0.0019, 'grad_norm': 0.18328797817230225, 'learning_rate': 1.3959731543624163e-05, 'epoch': 0.91}\n",
      "{'loss': 0.0018, 'grad_norm': 0.7156848907470703, 'learning_rate': 1.3736017897091724e-05, 'epoch': 0.94}\n",
      "{'loss': 0.0019, 'grad_norm': 0.12652824819087982, 'learning_rate': 1.3512304250559285e-05, 'epoch': 0.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95844a5a09e54c2f83d4d927806d1dac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.002390013076364994, 'eval_runtime': 165.4403, 'eval_samples_per_second': 16.012, 'eval_steps_per_second': 1.003, 'epoch': 1.0}\n",
      "{'loss': 0.0022, 'grad_norm': 0.12640808522701263, 'learning_rate': 1.3288590604026848e-05, 'epoch': 1.01}\n",
      "{'loss': 0.0017, 'grad_norm': 0.3174171447753906, 'learning_rate': 1.3064876957494407e-05, 'epoch': 1.04}\n",
      "{'loss': 0.0023, 'grad_norm': 0.26144808530807495, 'learning_rate': 1.284116331096197e-05, 'epoch': 1.07}\n",
      "{'loss': 0.0019, 'grad_norm': 0.2745983898639679, 'learning_rate': 1.2617449664429532e-05, 'epoch': 1.11}\n",
      "{'loss': 0.0017, 'grad_norm': 0.11088992655277252, 'learning_rate': 1.2393736017897093e-05, 'epoch': 1.14}\n",
      "{'loss': 0.0018, 'grad_norm': 0.12338884174823761, 'learning_rate': 1.2170022371364654e-05, 'epoch': 1.17}\n",
      "{'loss': 0.0017, 'grad_norm': 0.49450385570526123, 'learning_rate': 1.1946308724832217e-05, 'epoch': 1.21}\n",
      "{'loss': 0.0017, 'grad_norm': 0.2701670825481415, 'learning_rate': 1.1722595078299776e-05, 'epoch': 1.24}\n",
      "{'loss': 0.0017, 'grad_norm': 0.1683337539434433, 'learning_rate': 1.149888143176734e-05, 'epoch': 1.28}\n",
      "{'loss': 0.0016, 'grad_norm': 0.41725993156433105, 'learning_rate': 1.1275167785234899e-05, 'epoch': 1.31}\n",
      "{'loss': 0.0017, 'grad_norm': 0.269747257232666, 'learning_rate': 1.1051454138702462e-05, 'epoch': 1.34}\n",
      "{'loss': 0.0019, 'grad_norm': 0.2803669273853302, 'learning_rate': 1.0827740492170023e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0016, 'grad_norm': 0.38368359208106995, 'learning_rate': 1.0604026845637586e-05, 'epoch': 1.41}\n",
      "{'loss': 0.0016, 'grad_norm': 0.182883620262146, 'learning_rate': 1.0380313199105145e-05, 'epoch': 1.44}\n",
      "{'loss': 0.0015, 'grad_norm': 0.12697283923625946, 'learning_rate': 1.0156599552572708e-05, 'epoch': 1.48}\n",
      "{'loss': 0.0015, 'grad_norm': 0.14430741965770721, 'learning_rate': 9.93288590604027e-06, 'epoch': 1.51}\n",
      "{'loss': 0.0015, 'grad_norm': 0.41340142488479614, 'learning_rate': 9.70917225950783e-06, 'epoch': 1.54}\n",
      "{'loss': 0.0017, 'grad_norm': 0.3244858682155609, 'learning_rate': 9.485458612975392e-06, 'epoch': 1.58}\n",
      "{'loss': 0.0014, 'grad_norm': 0.11705116927623749, 'learning_rate': 9.261744966442953e-06, 'epoch': 1.61}\n",
      "{'loss': 0.0015, 'grad_norm': 0.1934083253145218, 'learning_rate': 9.038031319910516e-06, 'epoch': 1.64}\n",
      "{'loss': 0.0015, 'grad_norm': 0.17788466811180115, 'learning_rate': 8.814317673378077e-06, 'epoch': 1.68}\n",
      "{'loss': 0.0016, 'grad_norm': 0.13271263241767883, 'learning_rate': 8.590604026845638e-06, 'epoch': 1.71}\n",
      "{'loss': 0.0014, 'grad_norm': 0.2131660431623459, 'learning_rate': 8.3668903803132e-06, 'epoch': 1.74}\n",
      "{'loss': 0.0014, 'grad_norm': 0.5696993470191956, 'learning_rate': 8.14317673378076e-06, 'epoch': 1.78}\n",
      "{'loss': 0.0015, 'grad_norm': 0.08737337589263916, 'learning_rate': 7.919463087248322e-06, 'epoch': 1.81}\n",
      "{'loss': 0.0015, 'grad_norm': 0.2526409327983856, 'learning_rate': 7.695749440715883e-06, 'epoch': 1.85}\n",
      "{'loss': 0.0013, 'grad_norm': 0.253182053565979, 'learning_rate': 7.472035794183445e-06, 'epoch': 1.88}\n",
      "{'loss': 0.0015, 'grad_norm': 0.15885820984840393, 'learning_rate': 7.248322147651007e-06, 'epoch': 1.91}\n",
      "{'loss': 0.0015, 'grad_norm': 0.2517085373401642, 'learning_rate': 7.024608501118568e-06, 'epoch': 1.95}\n",
      "{'loss': 0.0015, 'grad_norm': 0.2733995318412781, 'learning_rate': 6.8008948545861304e-06, 'epoch': 1.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "726cf454b0464bd591cc8d2e8962053e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0038663328159600496, 'eval_runtime': 159.6187, 'eval_samples_per_second': 16.596, 'eval_steps_per_second': 1.04, 'epoch': 2.0}\n",
      "{'loss': 0.0015, 'grad_norm': 0.2808120846748352, 'learning_rate': 6.5771812080536925e-06, 'epoch': 2.01}\n",
      "{'loss': 0.0014, 'grad_norm': 0.16792817413806915, 'learning_rate': 6.353467561521254e-06, 'epoch': 2.05}\n",
      "{'loss': 0.0013, 'grad_norm': 0.09805745631456375, 'learning_rate': 6.129753914988815e-06, 'epoch': 2.08}\n",
      "{'loss': 0.0014, 'grad_norm': 0.24636541306972504, 'learning_rate': 5.906040268456377e-06, 'epoch': 2.11}\n",
      "{'loss': 0.0014, 'grad_norm': 0.25639012455940247, 'learning_rate': 5.682326621923938e-06, 'epoch': 2.15}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"sentence-transformers/paraphrase-distilroberta-base-v1\", num_labels=1)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    ")\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {results}\")\n",
    "\n",
    "# Visualize training and validation loss\n",
    "training_logs = trainer.state.log_history\n",
    "train_loss = [entry['loss'] for entry in training_logs if 'loss' in entry]\n",
    "eval_loss = [entry['eval_loss'] for entry in training_logs if 'eval_loss' in entry]\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "plt.plot(epochs, train_loss, label='Training Loss', marker='o')\n",
    "plt.plot(epochs, eval_loss, label='Validation Loss', marker='o', color='orange')\n",
    "plt.title(\"Training and Validation Loss Over Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix (assuming binary classification for simplicity)\n",
    "predictions = trainer.predict(val_dataset).predictions\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "true_labels = val_dataset['labels']\n",
    "\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Class 0', 'Class 1'])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Visualize embeddings\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    embeddings = model(torch.tensor(train_dataset['input_ids']).to(\"cpu\")).last_hidden_state[:, 0, :].numpy()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=train_dataset['labels'], cmap='viridis', alpha=0.7)\n",
    "plt.title(\"2D Visualization of Learned Embeddings\")\n",
    "plt.colorbar(label='Labels')\n",
    "plt.show()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine_tuned_paraphrase_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_paraphrase_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./fine_tuned_paraphrase_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_paraphrase_model\")\n",
    "\n",
    "# Function to compute relevance scores\n",
    "def compute_relevance_score(comment, title):\n",
    "    text_pair = comment + \" [SEP] \" + title\n",
    "    inputs = tokenizer(text_pair, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    score = torch.sigmoid(logits).item()  # Use sigmoid to convert to a probability score\n",
    "    return score\n",
    "\n",
    "# Example usage\n",
    "comment = \"What are the advantages of using MERN stack?\"\n",
    "title = \"Introduction to MERN Stack Development\"\n",
    "score = compute_relevance_score(comment, title)\n",
    "print(f\"Relevance score: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: \"'accelerate\"\n"
     ]
    }
   ],
   "source": [
    "pip install 'accelerate>=0.26.0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (2.5.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.20.1-cp39-cp39-win_amd64.whl (1.6 MB)\n",
      "     ---------------------------------------- 1.6/1.6 MB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.2.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from torchvision) (1.24.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\aditya agarwal\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.20.1\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
